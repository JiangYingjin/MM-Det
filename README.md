# [NeurIPS 2024] On Learning Multi-Modal Forgery Representation for Diffusion Generated Video Detection

This repository is the official implementation of [MM-Det](https://arxiv.org/abs/2410.23623) [NeurIPS 2024 Poster].

[![arxiv](https://img.shields.io/badge/arXiv-2310.23623-b31b1b.svg)](https://arxiv.org/abs/2410.23623)

- We develop an effective detector, MM-Det, based on multimodal forgery representation. 

- We release a benchmark for forgery detection on diffusion videos. We will provide the link sooner.

<table class="center">
    <tr>
    <td><img src="assets/overview_of_main_method.png"></td>
    </tr>
</table>


## Contents

- [Environment](#environment)
- [Dataset](#dataset)
- [Data Preparation](#data-preparation)
  - [Reconstruction Process](#reconstruction-process)
  - [Caching Multi-Modal Forgery Representation](#caching-multi-modal-forgery-representation)
- [Pretrained Weights](#pretrained-weights)
- [Training](#training)
- [Evaluation](#evaluation)

## Environment

1. Install basic packages
```bash
conda create -n MM_Det python=3.10
conda activate 
pip install -r requirements.txt
cd LLaVA
pip install -e .
```
2. For training cases, install additional packages
```bash
cd LLaVA
pip install --upgrade pip
pip install -e ".[train]"
pip install flash-attn==2.5.8 --no-build-isolation
```

## Dataset

### Diffusion Video Forensics (DVF)
We release Diffusion Video Forensics (DVF) as the benchmark for forgery video detection.

<table class="center">
    <tr>
    <td><img src="assets/dvf_dataset_samples.png"></td>
    </tr>
</table>

DVF contains 8 diffusion generative methods, including [Stable Diffusion](https://github.com/comfyanonymous/ComfyUI), [VideoCrafter1](https://github.com/AILab-CVC/VideoCrafter), [Zeroscope](https://huggingface.co/cerspense/zeroscope_v2_576w), [Sora](https://openai.com/index/sora/), [Pika](https://pika.art/), [OpenSora](https://github.com/hpcaitech/Open-Sora), [Stable Video](https://stability.ai/stable-video), and [Stable Video Diffusion](https://github.com/Stability-AI/generative-models).

<table class="center">
    <tr>
    <td><img src="assets/dvf_dataset_statistics.png"></td>
    </tr>
</table>

### Rich Forgery Reasoning Dataset

To finetune large multi-modal models on forgery detection, we construct a text-image paired description dataset called Rich Forgery Reasoning Dataset. This dataset contains 38k conversations on 1921 real frames and 3579 fake frames, with each description generated from Gemini v1.5 pro for forgery analysis. Real videos are from Youtube, while fake ones are generated by [Stable Video Diffusion](https://github.com/Stability-AI/generative-models). The original frames can be extracted from `youtube` and `stablevideodiffusion` in DVF.

Based on these descriptions, we build up an instruction-based dataset for fine-tuning. This dataset contains up to 38k conversations on 5,500 images, which can be achieved at [rfrd](https://drive.google.com/drive/folders/1buMlUlmxpBfmmF00eXMkzvdJKIXEApvw?usp=sharing).

`rfrd.json` contains the overall descriptions, while `rfrd_train.json` is a subset which only contains all training data without validation data to avoid data leakage.

#### Data Structure
Each element contains an image with a conversation based on forgery detection and reasoning.
```json
[
    {
      "id": "311b69de83b528413133ec406b8963d5ca9aa3274dff9d2f64c76adf82edf66d",
      "image": "stablevideodiffusion/1_fake/original/WoXBVCpy2C4_9.jpg",
      "conversations": [
          {
              "from": "human",
              "value": "<image>\nAs an experienced forgery inspection expert, please investigate whether the image is genuine or not. Investigate the legitimacy of the visual assets"
          },
          {
              "from": "gpt",
              "value": "The image shows a person in a red shirt who appears to be lying down on a table.  The subject's arm appears to be unnaturally stretched out, and the angle of the hand holding the flask is awkward."
          },
          {
              "from": "human",
              "value": "provide a further analysis on where it is real or fake."
          },
          {
              "from": "gpt",
              "value": "The image is likely a spoof image because the unnatural pose of the person in the red shirt suggests that the image has been manipulated.  It is possible that the image was created by combining different images to create a humorous or surreal scenario."
          }
      ]
    },
    ...
]
```

## Data Preparation

### Reconstruction Dataset

Based on the findings([DIRE](https://github.com/ZhendongWang6/DIRE)) that generative methods always fail to reconstruct details in real videos, we extend this method by utilizing a VQVAE trained on ImageNet to reconstruct each frame. The reconstructed data structure is as follows.

```
--$FRAME_DATA_ROOT
  | -- dataset A
    | -- class A1
      | -- original    # frame sequences for original videos
        | -- {video_id_1}_1.jpg
        ...
        | -- {video_id_M}_{frame_id_N}.jpg
      | -- recons    # frame sequences for reconstructed videos
        | -- {video_id_1}_1.jpg
        ...
        | -- {video_id_M}_{frame_id_N}.jpg
    | -- class A2
      | -- original    # frame sequences for original videos
        | -- {video_id_1}_1.jpg
        ...
        | -- {video_id_M}_{frame_id_N}.jpg
      | -- recons    # frame sequences for reconstructed videos
        | -- {video_id_1}_1.jpg
        ...
        | -- {video_id_M}_{frame_id_N}.jpg
  | -- dataset B
      ...
```

For reconstruction datasets of all videos in DVF, we will provide readily available paired dataset sooner for evaluation on MM-Det.

### Customized Preparation

For customized videos on training and inference, the data should be organized into the [reconstruction dataset structure](#reconstruction-dataset) first. We provide our pretrained VQVAE for reconstruction. Please download the [weights](https://drive.google.com/drive/folders/1RRNS8F7ETZWrcBu8fvB3pM9qHbmSEEzy?usp=sharing) at `vqvae/model.pt` and put it at `./weights/`. The reconstruction process is as follows.

1. Prepare the original videos in the following structure. 

```
-- $VIDEO_DATA_ROOT
  | -- dataset A
    | -- class A1    # e.g., 0_real
      | -- {video_1}.mp4/avi/...
      ...
      | -- {video_N}.mp4/avi/...
    | -- class A2    # e.g., 1_fake
      | -- {video_1}.mp4/avi/...
      ...
      | -- {video_N}.mp4/avi/...
  | -- dataset B
    | -- class B1
      | -- {video_1}.mp4/avi/...
      ...
      | -- {video_N}.mp4/avi/...
    | -- class B2
      | -- {video_1}.mp4/avi/...
      ...
      | -- {video_N}.mp4/avi/...
  ...
```

2. Run the following bash to convert videos into frame suquence and generate reconstructed frames.
```bash
python prepare_reconstructed_dataset.py -d $VIDEO_DATA_ROOT -o $RECONSTRUCTION_DATASET_ROOT
```

### Caching Multi-Modal Forgery Representation

Our method take adavantage of Multi-Modal Forgery Representation (MMFR) based on finetuned LLaVA-1.5 for forgery detection. Since the representation is fixed during training and inference, it is recommended to cache the representation before the overall training to reduce time cost. We provide a procedure for caching.

1. Prepare the dataset as [the reconstructed data structure](#reconstruction-process), where the data root is denoted as `$RECONSTRUCTION_DATASET_ROOT`
2. Run the following script to conduct inference on frames based on our finetuned LLaVA.

```bash
python prepare_mm_representations.py --cached-data-root $RECONSTRUCTION_DATASET_ROOT --output-dir $MM_REPRESENTATION_ROOT
```
3. The results will be saved as follows, where the MMFR of each class is cached and saved as a pth file.
```
| -- $MM_REPRESENTATION_ROOT
  | -- dataset A
    | -- class A1
      | -- mm_representation.pth
    | -- class A2
      | -- mm_representation.pth
  | -- dataset B
    | -- class B1
      | -- mm_representation.pth
    | -- class B2
      | -- mm_representation.pth
  ...
```

In each pth file, the MMFR for every frame is saved as:
```
{
  'frame_id_1.jpg': {
    'visual': visual_feature,
    'textual': textual_feature
  },
  ...
}
```


## Pre-trained Weights
We provide the pre-trained weights for our fine-tuned large multimodal model, llava-v1.5-Vicuna-7b from [LLaVA](https://github.com/haotian-liu/LLaVA), which is automatically downloaded. The overall weights for MM-Det without the LMM can be achieved from [weights](https://drive.google.com/drive/folders/1RRNS8F7ETZWrcBu8fvB3pM9qHbmSEEzy?usp=sharing) at `MM-Det/current_model.pth`. Please download and put the weights at `./weights/`.


## Training

### Finetuning Large Multi-modal Model
Our LMM branch is built upon [LLaVA](https://github.com/haotian-liu/LLaVA), with [llava-v1.5-Vicuna-7b](https://huggingface.co/liuhaotian/llava-v1.5-7b) set as the base model. Our fine-tuned LMM weights can be achieved [here](https://huggingface.co/sparklexfantasy/llava-7b-1.5-rfrd)(#pretrained-weights). 

We directly conduct the visual instruction tuning stage in [LLaVA](https://github.com/haotian-liu/LLaVA#train) on a gemini-generated instruction dataset [RFRD](#rich-forgery-reasoning-dataset). To reproduce our fine-tuned model, run

```bash
cd LLaVA
bash scripts/v1_5/finetune_task_llava-1.5-7b_rfrd.sh
```

After fine-tuning, to merge the lora weights with the base model, run
```
cd LLaVA
python scripts/merge_lora_weights.py --model-path $CHECKPOINT_PATH --model-base $BASE_MODEL_PATH --save-model-path $LMM_OUTPUT_PATH
```

If you plan to fine-tune the LMM based on customized dataset, you can start from [llava-v1.5-Vicuna-7b](https://huggingface.co/liuhaotian/llava-v1.5-7b) as the base model. Please following the [instruction](https://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md) of [LLaVA](https://github.com/haotian-liu/LLaVA) to prepare your dataset and conduct fine-tuning.

### Overall Training

Our ST backbone is based on Hybrid-ViT at [pytorch-image-models](https://github.com/huggingface/pytorch-image-models). Our model is based on `vit_base_resnet50_224_in21k`. To start training from a pretrained model, you can get the pretrained weights at [pytorch-image-models](https://github.com/huggingface/pytorch-image-models). We also provide a direct [link](https://drive.google.com/drive/folders/1RRNS8F7ETZWrcBu8fvB3pM9qHbmSEEzy?usp=sharing) at `ViT/vit_base_r50_s16_224.orig_in21k`. Please put downloaded weights at `./weights/`.

Run the following scripts `launch-train.bash` for an overall training on our model. It is recommended to first cache Multi-Modal Forgery Representations at `$MM_REPRESENTATION_ROOT`. In this case, `--cache-mm` is specified, and LMM branch will not be loaded to save huge computational costs and memory usage. `--fix-split` is specified to load training and validation datasets from a fixed split to prevent data leakage between fine-tuning the LMM and the overall training. We make sure there is no overlap between training and validation data in both stages.

```bash
python train.py \
--data-root $RECONSTRUCTION_DATASET_ROOT \
--classes youtube stablevideodiffusion \
--fix-split \
--split ./splits \
--cache-mm \
--mm-root $MM_REPRESENTATION_ROOT \
--expt $EXPT_NAME \
```

## Evaluation

For datasets in DVF, the recostructed datasets as well as the cached MMFR will be provided. (We will make it available soon.) Set `$RECONSTRUCTION_DATASET_ROOT` as `DVF_recons` and `$MM_REPRESENTATION_ROOT` as `mm_representations`.

For customized datasets, prepare the test dataset frames as well as the cached MMFR by following [Data Preparation](#data-preparation). Set `$RECONSTRUCTION_DATASET_ROOT` and `$MM_REPRESENTATION_ROOT` to both data roots. `--cache-mm` is also recommended for save the computational and memory cost of LMM branch.

Make sure the pretrained weights are organize at `./weights.` Then, run the following script for testing on 7 datasets, respectively. Since the entire evaluation is time-costing, `sample-size` can be specified (e.g., 1,000) to reduce time by conducting inference only on limited (1,000) videos. To finish the entire evaluation, please set `sample-size` as `-1`.

```bash
python test.py \
--classes videocrafter1 zeroscope opensora sora pika stablediffusion stablevideo \
--ckpt ./weights/MM-Det/current_model.pth \
--cache-mm \
# when sample-size > 0, only [sample-size] videos are evaluated for each dataset.
--sample-size -1
```

## Acknowledgement

We express our sincere appreciation to the following projects.

- [LLaVA](https://github.com/haotian-liu/LLaVA)
- [pytorch-image-models](https://github.com/huggingface/pytorch-image-models)
- [pytorch-vqvae](https://github.com/ritheshkumar95/pytorch-vqvae)
- [Stable Diffusion](https://github.com/comfyanonymous/ComfyUI)
- [VideoCrafter1](https://github.com/AILab-CVC/VideoCrafter)
- [Zeroscope](https://huggingface.co/cerspense/zeroscope_v2_576w)
- [OpenSora](https://github.com/hpcaitech/Open-Sora)
- [Stable Video Diffusion](https://github.com/Stability-AI/generative-models).

## Citation

@misc{song2024learningmultimodalforgeryrepresentation,
      title={On Learning Multi-Modal Forgery Representation for Diffusion Generated Video Detection}, 
      author={Xiufeng Song and Xiao Guo and Jiache Zhang and Qirui Li and Lei Bai and Xiaoming Liu and Guangtao Zhai and Xiaohong Liu},
      year={2024},
      eprint={2410.23623},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.23623}, 
}