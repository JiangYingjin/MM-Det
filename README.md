# [NeurIPS 2024] On Learning Multi-Modal Forgery Representation for Diffusion Generated Video Detection

- We develop an effective detector MM-Det based on multimodal forgery representation. [placeholder for internal link]

- We release a benchmark for forgery detection on diffusion videos. [placeholder for external link]

[placeholder for the main graph]

[placeholder for internal/external resourcec links]

### TODO:

- [ X ] Introduction / Readme
- [ X ] Environment
- [ X ] Reconstruction Process
- [ X ] Instruction Finetuning
- [ X ] pre cached dump llava
- [ X ] Baseline Finetuning
- [ X ] Baseline testing
- [ X ] Other basic information (i.e. citation, acknowledgement, ..)

## Contents

- [Environment](#environment)
- [Dataset](#dataset)
- [Data Preparation](#data-preparation)
  - [Reconstruction Process](#reconstruction-process)
  - [Caching Multi-Modal Forgery Representation](#caching-multi-modal-forgery-representation)
- [Pretrained Weights](#pretrained-weights)
- [Training](#training)
- [Evaluation](#evaluation)

## Environment

1. Install basic packages
```bash
conda create -n MM_Det python=3.10
conda activate 
pip install -r requirements.txt
cd LLaVA
pip install -e .
```
2. For training cases, install additional packages
```bash
cd LLaVA
pip install --upgrade pip
pip install -e ".[train]"
pip install flash-attn==2.5.8 --no-build-isolation
```

## Dataset

### Diffusion Video Forensics (DVF)
We release Diffusion Video Forensics (DVF) as the benchmark for forgery video detection.

[Detailed information for DVF]

### Rich Forgery Reasoning Dataset

To finetune large multi-modal models on forgery detection, we construct a text-image paired description dataset called Rich Forgery Reasoning Dataset. This dataset contains 5556 descriptions on 2076 real frames and 3480 fake frames, with each description generated from Gemini v1.5 pro for forgery analysis. Real videos are from Youtube, while fake ones are generated by [Stable Video Diffusion](https://github.com/Stability-AI/generative-models). Raw descriptions can be achieved at [placeholder].

#### Data Structure
Each element contains the image path and an analysis on its authenticity.
```json
[
  {
    "image": "KjJDT0ZaYEw_7.jpg",
    "response": "The image shows a vibrant purple flower and a white flower against a backdrop of lush green leaves. The colors are highly saturated, especially the purple of the flower, which appears almost unnatural. The leaves are arranged in a way that feels repetitive and staged, lacking the random distribution found in real-life foliage. There's an unnatural clarity to the image, with no blurring or imperfections that would be present in a natural setting. \n\nThe excessive saturation and the contrived arrangement of the foliage are strong indicators that the image has been manipulated, making it likely a fabricated image rather than a genuine photograph.\n "
  },
  ...
]
```

Based on these descriptions, we build up an instruction-based dataset for fine-tuning. This dataset contains up to 33.6k conversations, which can be achieved at [placeholder].

#### Data Structure
Each element contains an image with a conversation based on forgery detection and reasoning.
```json
[
    {
      "id": "311b69de83b528413133ec406b8963d5ca9aa3274dff9d2f64c76adf82edf66d",
      "image": "1_fake/original/WoXBVCpy2C4_9.jpg",
      "conversations": [
          {
              "from": "human",
              "value": "<image>\nAs an experienced forgery inspection expert, please investigate whether the image is genuine or not. Investigate the legitimacy of the visual assets"
          },
          {
              "from": "gpt",
              "value": "The image shows a person in a red shirt who appears to be lying down on a table.  The subject's arm appears to be unnaturally stretched out, and the angle of the hand holding the flask is awkward."
          },
          {
              "from": "human",
              "value": "provide a further analysis on where it is real or fake."
          },
          {
              "from": "gpt",
              "value": "The image is likely a spoof image because the unnatural pose of the person in the red shirt suggests that the image has been manipulated.  It is possible that the image was created by combining different images to create a humorous or surreal scenario."
          }
      ]
    },
    ...
]
```

## Data Preparation

### Reconstruction Process

Based on the findings([DIRE](https://github.com/ZhendongWang6/DIRE)) that generative methods always fail to reconstruct details in real videos, we extend this method by utilizing a VQVAE trained on ImageNet to reconstruct each frame. For videos in Diffusion Video Forensics, we provide readily available data pair (original frames, reconstructed frames) in [placeholder for dataset]. The reconstructed data structure is as follows.

```
--$FRAME_DATA_ROOT
  | -- class A
    | -- original    # frame sequences for original videos
      | -- {video_id_1}_1.jpg
      ...
      | -- {video_id_M}_{frame_id_N}.jpg
    | -- recons    # frame sequences for reconstructed videos
      | -- {video_id_1}_1.jpg
      ...
      | -- {video_id_M}_{frame_id_N}.jpg
  | -- class B
      ...
```

For customized videos on training and inference, we provide our pretrained VQVAE based on [placeholder for external link] for reconstruction. The weights can be achieved at `weights/vqvae/model.pt`. The reconstruction process is as follows.

1. Prepare the original videos in the following structure. 

```
-- $VIDEO_DATA_ROOT
  | -- class A
    | -- {video_A1}.mp4/avi/...
    ...
    | -- {video_AN}.mp4/avi/...
  | -- class B
    | -- {video_B1}.mp4/avi/...
    | -- {video_BN}.mp4/avi/...
  ...
```

2. Run the following bash to convert videos into frame suquence and generate reconstructed frames.
```bash
python scripts/extract_video_frames.py -d $VIDEO_DATA_ROOT -o $OUTPUT_PATH
```
### Caching Multi-Modal Forgery Representation

Our method take adavantage of Multi-Modal Forgery Representation (MMFR) based on finetuned LLaVA-1.5 for forgery detection. Since the representation is fixed during training and inference, it is recommended to cache the representation before the overall training to reduce time cost. We provide a script for caching.

1. Prepare the dataset as [the reconstructed data structure](#reconstruction-process).
2. Run the following script to conduct inference on frames based on our finetuned LLaVA.

```bash
Under test.
```
3. The results will be saved as follows, where the MMFR of each class is cached and saved as a pth file.
```
| -- $CACHE_DIR
  | -- class A
    | -- llava_representation.pth
  | -- class B
    | -- llava_representation.pth
  ...
```


## Pre-trained Weights
We provide the pre-trained weights for our fine-tuned large multimodal model, llava-v1.5-Vicuna-7b from [LLaVA](https://github.com/haotian-liu/LLaVA) at [placeholder for weight link], which is automatically downloaded. The rest weights in our model is achieved at [placeholder for weight link]. Please put manually downloaded weights at `./weights`.

```
| -- weights
  | -- mm_det
    | -- ...
```

## Training

### Finetuning Large Multimodal Model
Our LMM branch is built upon [LLaVA](https://github.com/haotian-liu/LLaVA), with [llava-v1.5-Vicuna-7b](https://huggingface.co/liuhaotian/llava-v1.5-7b) set as the base model. Our fine-tuned LMM weights are [here](#pretrained-weights). 

We directly conduct the visual instruction tuning stage in [LLaVA](https://github.com/haotian-liu/LLaVA#train) on a 33k gemini-generated instruction data: `./LLaVA/playground/[placeholder for data]`. To reproduce our fine-tuned model, run

```bash
cd LLaVA
bash scripts/v1_5/finetune_task_7b_svd_33k.sh
```

After fine-tuning, to merge the lora weights with the base model, run
```
cd LLaVA
python scripts/merge_lora_weights.py --model-path $CHECKPOINT_PATH --model-base $BASE_MODEL_PATH --save-model-path $LMM_OUTPUT_PATH
```

If you plan to fine-tune the LMM based on customized dataset, you can start from [llava-v1.5-Vicuna-7b](https://huggingface.co/liuhaotian/llava-v1.5-7b) as the base model. Please following the [instruction](https://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md) of [LLaVA](https://github.com/haotian-liu/LLaVA) to prepare your dataset and conduct fine-tuning.

### Overall Training

Run the following scripts for overall training on our model.

```bash
python train.py
```

## Evaluation

Prepare the test dataset frames as well as the cached MMFR by following [Data Preparation](#data-preparation).

Run the following script for testing.

```bash
python test.py
```

## Acknowledgement

We express our sincere appreciation to the following projects.

- [LLaVA](https://github.com/haotian-liu/LLaVA)
- [pytorch-image-models](https://github.com/huggingface/pytorch-image-models)
- [pytorch-vqvae](https://github.com/ritheshkumar95/pytorch-vqvae)

## Citation

[placeholder for citation]